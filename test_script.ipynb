{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noelkj/miniconda3/envs/ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "sys.path.append('data/')\n",
    "sys.path.append('models/')\n",
    "from my_dataset import MyDataset00000000 as MyDataset\n",
    "from my_model import MyModel00000000 as MyModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/noelkj/Documents/GitHub/AI612-project2/train/testdata/ to cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid magic number; corrupt file?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Create the dataset and a data loader\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m dataset \u001b[39m=\u001b[39m MyDataset(data_path\u001b[39m=\u001b[39;49mdata_path)\n\u001b[1;32m      9\u001b[0m data_loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Instantiate the model\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/AI612-project2/data/my_dataset.py:19\u001b[0m, in \u001b[0;36mMyDataset00000000.__init__\u001b[0;34m(self, data_path, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading data from \u001b[39m\u001b[39m{\u001b[39;00mdata_path\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, \u001b[39m'\u001b[39;49m\u001b[39mfeatures.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m), map_location\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m'\u001b[39m\u001b[39mlabels.pkl\u001b[39m\u001b[39m'\u001b[39m), map_location\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m    712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m--> 713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/serialization.py:922\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    920\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    921\u001b[0m \u001b[39mif\u001b[39;00m magic_number \u001b[39m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m--> 922\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid magic number; corrupt file?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    923\u001b[0m protocol_version \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    924\u001b[0m \u001b[39mif\u001b[39;00m protocol_version \u001b[39m!=\u001b[39m PROTOCOL_VERSION:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid magic number; corrupt file?"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "data_path = '/Users/noelkj/Documents/GitHub/AI612-project2/train/testdata/'\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Create the dataset and a data loader\n",
    "dataset = MyDataset(data_path=data_path)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel()\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (lstm): LSTM(128, 512, batch_first=True)\n",
      "  (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (task_layers): ModuleDict(\n",
      "    (short_mortality): Linear(in_features=512, out_features=2, bias=True)\n",
      "    (long_mortality): Linear(in_features=512, out_features=2, bias=True)\n",
      "    (readmission): Linear(in_features=512, out_features=2, bias=True)\n",
      "    (diagnosis): Linear(in_features=512, out_features=34, bias=True)\n",
      "    (short_los): Linear(in_features=512, out_features=2, bias=True)\n",
      "    (long_los): Linear(in_features=512, out_features=2, bias=True)\n",
      "    (final_acuity): Linear(in_features=512, out_features=6, bias=True)\n",
      "    (imminent_discharge): Linear(in_features=512, out_features=6, bias=True)\n",
      "    (creatinine_level): Linear(in_features=512, out_features=5, bias=True)\n",
      "    (bilirubin_level): Linear(in_features=512, out_features=5, bias=True)\n",
      "    (platelet_level): Linear(in_features=512, out_features=5, bias=True)\n",
      "    (wbc_level): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmultitask_loss(outputs, targets)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[37], line 92\u001b[0m, in \u001b[0;36mMyModel.multitask_loss\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     76\u001b[0m loss_functions \u001b[39m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshort_mortality\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mcross_entropy,\n\u001b[1;32m     78\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlong_mortality\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mcross_entropy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwbc_level\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mcross_entropy,\n\u001b[1;32m     89\u001b[0m }\n\u001b[1;32m     91\u001b[0m \u001b[39m# Calculate loss for each task\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m losses \u001b[39m=\u001b[39m {task: loss_functions[task](predictions[task], targets[task]) \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m predictions\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m     94\u001b[0m \u001b[39m# Combine the losses\u001b[39;00m\n\u001b[1;32m     95\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses\u001b[39m.\u001b[39mvalues())\n",
      "Cell \u001b[0;32mIn[37], line 92\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m loss_functions \u001b[39m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshort_mortality\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mcross_entropy,\n\u001b[1;32m     78\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlong_mortality\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mcross_entropy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwbc_level\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mcross_entropy,\n\u001b[1;32m     89\u001b[0m }\n\u001b[1;32m     91\u001b[0m \u001b[39m# Calculate loss for each task\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m losses \u001b[39m=\u001b[39m {task: loss_functions[task](predictions[task], targets[task]) \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m predictions\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m     94\u001b[0m \u001b[39m# Combine the losses\u001b[39;00m\n\u001b[1;32m     95\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses\u001b[39m.\u001b[39mvalues())\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        # Unpack the data and move it to the appropriate device\n",
    "        inputs = batch[\"data\"].to(model.device)\n",
    "        targets = batch[\"label\"].to(model.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = model.multitask_loss(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
