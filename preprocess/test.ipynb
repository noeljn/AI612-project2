{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>DBSOURCE</th>\n",
       "      <th>FIRST_CAREUNIT</th>\n",
       "      <th>LAST_CAREUNIT</th>\n",
       "      <th>FIRST_WARDID</th>\n",
       "      <th>LAST_WARDID</th>\n",
       "      <th>INTIME</th>\n",
       "      <th>OUTTIME</th>\n",
       "      <th>LOS</th>\n",
       "      <th>...</th>\n",
       "      <th>diagnosisstring</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>first_careunit</th>\n",
       "      <th>last_careunit</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>los</th>\n",
       "      <th>anchor_age</th>\n",
       "      <th>dod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1027140.0</td>\n",
       "      <td>3011529.0</td>\n",
       "      <td>carevue</td>\n",
       "      <td>CCU</td>\n",
       "      <td>CCU</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2152-07-23 05:34:12</td>\n",
       "      <td>2152-07-24 18:37:32</td>\n",
       "      <td>1.5440</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1027482.0</td>\n",
       "      <td>3011175.0</td>\n",
       "      <td>carevue</td>\n",
       "      <td>MICU</td>\n",
       "      <td>MICU</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2162-03-20 17:22:48</td>\n",
       "      <td>2162-03-22 18:26:10</td>\n",
       "      <td>2.0440</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1011746.0</td>\n",
       "      <td>3013234.0</td>\n",
       "      <td>carevue</td>\n",
       "      <td>MICU</td>\n",
       "      <td>MICU</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2107-07-28 17:22:31</td>\n",
       "      <td>2107-08-01 11:03:44</td>\n",
       "      <td>3.7370</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1010325.0</td>\n",
       "      <td>3022526.0</td>\n",
       "      <td>metavision</td>\n",
       "      <td>MICU</td>\n",
       "      <td>MICU</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2145-02-06 02:15:08</td>\n",
       "      <td>2145-02-07 18:47:37</td>\n",
       "      <td>1.6892</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1012708.0</td>\n",
       "      <td>3035542.0</td>\n",
       "      <td>metavision</td>\n",
       "      <td>SICU</td>\n",
       "      <td>SICU</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2175-02-20 07:41:28</td>\n",
       "      <td>2175-02-23 10:21:01</td>\n",
       "      <td>3.1108</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  ICUSTAY_ID    DBSOURCE FIRST_CAREUNIT LAST_CAREUNIT  \\\n",
       "0   1027140.0   3011529.0     carevue            CCU           CCU   \n",
       "1   1027482.0   3011175.0     carevue           MICU          MICU   \n",
       "2   1011746.0   3013234.0     carevue           MICU          MICU   \n",
       "3   1010325.0   3022526.0  metavision           MICU          MICU   \n",
       "4   1012708.0   3035542.0  metavision           SICU          SICU   \n",
       "\n",
       "   FIRST_WARDID  LAST_WARDID               INTIME             OUTTIME     LOS  \\\n",
       "0          57.0         57.0  2152-07-23 05:34:12 2152-07-24 18:37:32  1.5440   \n",
       "1          12.0         12.0  2162-03-20 17:22:48 2162-03-22 18:26:10  2.0440   \n",
       "2          52.0         52.0  2107-07-28 17:22:31 2107-08-01 11:03:44  3.7370   \n",
       "3          52.0         52.0  2145-02-06 02:15:08 2145-02-07 18:47:37  1.6892   \n",
       "4          33.0         33.0  2175-02-20 07:41:28 2175-02-23 10:21:01  3.1108   \n",
       "\n",
       "   ... diagnosisstring subject_id stay_id first_careunit last_careunit  \\\n",
       "0  ...             NaN        NaN     NaN            NaN           NaN   \n",
       "1  ...             NaN        NaN     NaN            NaN           NaN   \n",
       "2  ...             NaN        NaN     NaN            NaN           NaN   \n",
       "3  ...             NaN        NaN     NaN            NaN           NaN   \n",
       "4  ...             NaN        NaN     NaN            NaN           NaN   \n",
       "\n",
       "   intime  outtime  los  anchor_age  dod  \n",
       "0     NaT      NaT  NaN         NaN  NaT  \n",
       "1     NaT      NaT  NaN         NaN  NaT  \n",
       "2     NaT      NaT  NaN         NaN  NaT  \n",
       "3     NaT      NaT  NaN         NaN  NaT  \n",
       "4     NaT      NaT  NaN         NaN  NaT  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_pickle('../train/pooled_df.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SUBJECT_ID', 'ICUSTAY_ID', 'DBSOURCE', 'FIRST_CAREUNIT',\n",
      "       'LAST_CAREUNIT', 'FIRST_WARDID', 'LAST_WARDID', 'INTIME', 'OUTTIME',\n",
      "       'LOS', 'GENDER', 'DOB', 'DOD', 'DOD_HOSP', 'DOD_SSN', 'EXPIRE_FLAG',\n",
      "       'age', 'readmission', 'mortality', 'los_3day', 'los_7day', 'ICD9_CODE',\n",
      "       '12h_obs', '24h_obs', 'ID', 'code_name', 'code_offset', 'value', 'uom',\n",
      "       'code_order', 'seq_len', 'patienthealthsystemstayid', 'gender',\n",
      "       'ethnicity', 'hospitalid', 'wardid', 'apacheadmissiondx',\n",
      "       'admissionheight', 'hospitaladmittime24', 'hospitaladmitoffset',\n",
      "       'hospitaladmitsource', 'hospitaldischargeyear',\n",
      "       'hospitaldischargetime24', 'hospitaldischargeoffset',\n",
      "       'hospitaldischargelocation', 'hospitaldischargestatus', 'unittype',\n",
      "       'unitadmittime24', 'unitadmitsource', 'unitvisitnumber', 'unitstaytype',\n",
      "       'admissionweight', 'dischargeweight', 'unitdischargetime24',\n",
      "       'unitdischargeoffset', 'unitdischargelocation', 'unitdischargestatus',\n",
      "       'uniquepid', 'losday', 'diagnosisstring', 'subject_id', 'stay_id',\n",
      "       'first_careunit', 'last_careunit', 'intime', 'outtime', 'los',\n",
      "       'anchor_age', 'dod'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noelkj/miniconda3/envs/ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def embed(input_text):\n",
    "    tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    embeddings = model(**tokens).last_hidden_state\n",
    "    pooled_embeddings = embeddings.mean(dim=1)\n",
    "    return pooled_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_name Anion Gap value 11.0 uom mEq/L\n",
      "torch.Size([1, 768])\n",
      "code_name Bicarbonate value 25.0 uom mEq/L\n",
      "torch.Size([1, 768])\n",
      "code_name Bilirubin, Total value 0.3 uom mg/dL\n",
      "torch.Size([1, 768])\n",
      "code_name Cholesterol Ratio (Total/HDL) value 5.9 uom Ratio\n",
      "torch.Size([1, 768])\n",
      "code_name Cholesterol, HDL value 31.0 uom mg/dL\n",
      "torch.Size([1, 768])\n",
      "code_name Cholesterol, LDL, Calculated value 108.0 uom mg/dL\n",
      "torch.Size([1, 768])\n",
      "code_name Cholesterol, Total value 183.0 uom mg/dL\n",
      "torch.Size([1, 768])\n",
      "code_name Creatinine value 0.7 uom mg/dL\n",
      "torch.Size([1, 768])\n",
      "code_name Magnesium value 1.7 uom mg/dL\n",
      "torch.Size([1, 768])\n",
      "code_name Potassium value 4.1 uom mEq/L\n",
      "torch.Size([1, 768])\n",
      "code_name Sodium value 143.0 uom mEq/L\n",
      "torch.Size([1, 768])\n",
      "code_name Urea Nitrogen value 13.0 uom mg/dL\n",
      "torch.Size([1, 768])\n",
      "code_name INR(PT) value 1.0 uom  \n",
      "torch.Size([1, 768])\n",
      "code_name MCHC value 34.4 uom %\n",
      "torch.Size([1, 768])\n",
      "code_name MCV value 93.0 uom fL\n",
      "torch.Size([1, 768])\n",
      "code_name Platelet Count value 212.0 uom K/uL\n",
      "torch.Size([1, 768])\n",
      "code_name PT value 12.5 uom sec\n",
      "torch.Size([1, 768])\n",
      "code_name RDW value 12.1 uom %\n",
      "torch.Size([1, 768])\n",
      "code_name Potassium value 4.3 uom mEq/L\n",
      "torch.Size([1, 768])\n",
      "code_name Platelet Count value 230.0 uom K/uL\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n",
      "torch.Size([1, 768])\n",
      "code_name 0.0 value 0.0 uom 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# Test with first 10 rows\u001b[39;00m\n\u001b[1;32m     18\u001b[0m df_test \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mhead(\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m df_test \u001b[39m=\u001b[39m embed_data(df_test)\n\u001b[1;32m     20\u001b[0m df_test\u001b[39m.\u001b[39mhead()\n\u001b[1;32m     22\u001b[0m \u001b[39m# Save to pickle\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#df_test.to_pickle('df_test.pkl')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36membed_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      7\u001b[0m input_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcode_name \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mcode_name\u001b[39m\u001b[39m'\u001b[39m][k][n]) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m value \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m][k][n]) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m uom \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39muom\u001b[39m\u001b[39m'\u001b[39m][k][n])\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(input_text)\n\u001b[0;32m----> 9\u001b[0m embed_input \u001b[39m=\u001b[39m embed(input_text)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(embed_input\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m exit()\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36membed\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed\u001b[39m(input_text):\n\u001b[1;32m     11\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer(input_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     embeddings \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokens)\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     13\u001b[0m     pooled_embeddings \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m pooled_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1007\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1008\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1009\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1015\u001b[0m     embedding_output,\n\u001b[1;32m   1016\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1017\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1018\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1019\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1020\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1021\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1022\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1023\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1024\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1027\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    594\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    595\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    596\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m    605\u001b[0m         attention_mask,\n\u001b[1;32m    606\u001b[0m         layer_head_mask,\n\u001b[1;32m    607\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    609\u001b[0m         past_key_value,\n\u001b[1;32m    610\u001b[0m         output_attentions,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    614\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:531\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    528\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    529\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 531\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    532\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    534\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    536\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:544\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    543\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 544\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    545\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:456\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 456\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    457\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    458\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def embed_data(df):\n",
    "    # Create empty numpy array with len(df) rows and 256 columns\n",
    "    df_embed = []\n",
    "    for k in range(len(df)):\n",
    "        df_embed.append([])\n",
    "        for n in range(len(df['code_name'][k])):\n",
    "            input_text = 'code_name ' + str(df['code_name'][k][n]) + ' value ' + str(df['value'][k][n]) + ' uom ' + str(df['uom'][k][n])\n",
    "            print(input_text)\n",
    "            embed_input = embed(input_text)\n",
    "            print(embed_input.shape)\n",
    "            # Save to df\n",
    "            df_embed[k].append(embed_input)\n",
    "    return pd.DataFrame(df_embed)\n",
    "\n",
    "# Test with first 10 rows\n",
    "df_test = df.head(2)\n",
    "df_test = embed_data(df_test)\n",
    "df_test.head()\n",
    "\n",
    "# Save to pickle\n",
    "#df_test.to_pickle('df_test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
